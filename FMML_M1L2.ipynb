{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akhilvanama136/Fmml_module/blob/main/FMML_M1L2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine Learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2\n",
        "\n",
        "In this lab, we will show a part of the ML pipeline by using the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district. We will use the scikit-learn library to load the data and perform some basic data preprocessing and model training. We will also show how to evaluate the model using some common metrics, split the data into training and testing sets, and use cross-validation to get a better estimate of the model's performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = np.random.default_rng(seed=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LpqjN991GGJ",
        "outputId": "be0e0f0b-2809-4bee-9087-a5456ba67c09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ".. _california_housing_dataset:\n",
            "\n",
            "California Housing dataset\n",
            "--------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            ":Number of Instances: 20640\n",
            "\n",
            ":Number of Attributes: 8 numeric, predictive attributes and the target\n",
            "\n",
            ":Attribute Information:\n",
            "    - MedInc        median income in block group\n",
            "    - HouseAge      median house age in block group\n",
            "    - AveRooms      average number of rooms per household\n",
            "    - AveBedrms     average number of bedrooms per household\n",
            "    - Population    block group population\n",
            "    - AveOccup      average number of household members\n",
            "    - Latitude      block group latitude\n",
            "    - Longitude     block group longitude\n",
            "\n",
            ":Missing Attribute Values: None\n",
            "\n",
            "This dataset was obtained from the StatLib repository.\n",
            "https://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n",
            "\n",
            "The target variable is the median house value for California districts,\n",
            "expressed in hundreds of thousands of dollars ($100,000).\n",
            "\n",
            "This dataset was derived from the 1990 U.S. census, using one row per census\n",
            "block group. A block group is the smallest geographical unit for which the U.S.\n",
            "Census Bureau publishes sample data (a block group typically has a population\n",
            "of 600 to 3,000 people).\n",
            "\n",
            "A household is a group of people residing within a home. Since the average\n",
            "number of rooms and bedrooms in this dataset are provided per household, these\n",
            "columns may take surprisingly large values for block groups with few households\n",
            "and many empty houses, such as vacation resorts.\n",
            "\n",
            "It can be downloaded/loaded using the\n",
            ":func:`sklearn.datasets.fetch_california_housing` function.\n",
            "\n",
            ".. rubric:: References\n",
            "\n",
            "- Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n",
            "  Statistics and Probability Letters, 33 (1997) 291-297\n",
            "\n"
          ]
        }
      ],
      "source": [
        "dataset = datasets.fetch_california_housing()\n",
        "# Dataset description\n",
        "print(dataset.DESCR)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PCe1VNftevgE"
      },
      "source": [
        "Given below are the list of target values. These correspond to the house value derived considering all the 8 input features and are continuous values. We should use regression models to predict these values but we will start with a simple classification model for the sake of simplicity. We need to just round off the values to the nearest integer and use a classification model to predict the house value."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8K0ggBOevgE",
        "outputId": "a059badf-34e2-484c-e02b-60f57fa6a2cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Orignal target values: [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n",
            "Target values after conversion: [4 3 3 ... 0 0 0]\n",
            "Input variables shape: (20640, 8)\n",
            "Output variables shape: (20640,)\n"
          ]
        }
      ],
      "source": [
        "print(\"Orignal target values:\", dataset.target)\n",
        "\n",
        "dataset.target = dataset.target.astype(int)\n",
        "\n",
        "print(\"Target values after conversion:\", dataset.target)\n",
        "print(\"Input variables shape:\", dataset.data.shape)\n",
        "print(\"Output variables shape:\", dataset.target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "The simplest model to use for classification is the K-Nearest Neighbors model. We will use this model to predict the house value with a K value of 1. We will also use the accuracy metric to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "outputs": [],
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and a query point\n",
        "    and returns the predicted label for the query point using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    query: numpy array of shape (d,) where d is the number of features\n",
        "\n",
        "    returns: the predicted label for the query point which is the label of the training data which is closest to the query point\n",
        "    \"\"\"\n",
        "    diff = (\n",
        "        traindata - query\n",
        "    )  # find the difference between features. Numpy automatically takes care of the size here\n",
        "    sq = diff * diff  # square the differences\n",
        "    dist = sq.sum(1)  # add up the squares\n",
        "    label = trainlabel[np.argmin(dist)]\n",
        "    return label\n",
        "\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the nearest neighbour algorithm\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is the label of the training data which is closest to each test point\n",
        "    \"\"\"\n",
        "    predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "outputs": [],
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "    \"\"\"\n",
        "    This function takes in the training data, training labels and test data\n",
        "    and returns the predicted labels for the test data using the random classifier algorithm\n",
        "\n",
        "    In reality, we don't need these arguments but we are passing them to keep the function signature consistent with other classifiers\n",
        "\n",
        "    traindata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    trainlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    testdata: numpy array of shape (m,d) where m is the number of test samples and d is the number of features\n",
        "\n",
        "    returns: the predicted labels for the test data which is a random label from the training data\n",
        "    \"\"\"\n",
        "\n",
        "    classes = np.unique(trainlabel)\n",
        "    rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "    predlabel = classes[rints]\n",
        "    return predlabel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "We need a metric to evaluate the performance of the model. Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm. We will use the accuracy metric to evaluate and compate the performance of the K-Nearest Neighbors model and the random classifier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "outputs": [],
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "    \"\"\"\n",
        "    This function takes in the ground-truth labels and predicted labels\n",
        "    and returns the accuracy of the classifier\n",
        "\n",
        "    gtlabel: numpy array of shape (n,) where n is the number of samples\n",
        "    predlabel: numpy array of shape (n,) where n is the number of samples\n",
        "\n",
        "    returns: the accuracy of the classifier which is the number of correct predictions divided by the total number of predictions\n",
        "    \"\"\"\n",
        "    assert len(gtlabel) == len(\n",
        "        predlabel\n",
        "    ), \"Length of the ground-truth labels and predicted labels should be the same\"\n",
        "    correct = (\n",
        "        gtlabel == predlabel\n",
        "    ).sum()  # count the number of times the groundtruth label is equal to the predicted label.\n",
        "    return correct / len(gtlabel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability. We will use this function to split the dataset into training and testing sets. We will use the training set to train the model and the testing set to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "outputs": [],
      "source": [
        "def split(data, label, percent):\n",
        "    # generate a random number for each sample\n",
        "    rnd = rng.random(len(label))\n",
        "    split1 = rnd < percent\n",
        "    split2 = rnd >= percent\n",
        "\n",
        "    split1data = data[split1, :]\n",
        "    split1label = label[split1]\n",
        "    split2data = data[split2, :]\n",
        "    split2label = label[split2]\n",
        "    return split1data, split1label, split2data, split2label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBZkHBLJ1iU-",
        "outputId": "765ed768-4813-4311-ff8e-ecda02ec00df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples: 4144\n",
            "Number of train samples: 16496\n",
            "Percent of test data: 20.07751937984496 %\n"
          ]
        }
      ],
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(\n",
        "    dataset.data, dataset.target, 20 / 100\n",
        ")\n",
        "print(\"Number of test samples:\", len(testlabel))\n",
        "print(\"Number of train samples:\", len(alltrainlabel))\n",
        "print(\"Percent of test data:\", len(testlabel) * 100 / len(dataset.target), \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "outputs": [],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DBlZDTHUFTZx",
        "outputId": "865c389b-3255-46d0-d815-7c792d090d86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training accuracy using nearest neighbour algorithm: 100.0 %\n",
            "Training accuracy using random classifier:  16.4375808538163 %\n"
          ]
        }
      ],
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using nearest neighbour algorithm:\", trainAccuracy*100, \"%\")\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Training accuracy using random classifier: \", trainAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case. This is because the random classifier randomly assigns a label to each sample and the probability of assigning the correct label is 1/(number of classes). Let us predict the labels for our validation set and get the accuracy. This accuracy is a good estimate of the accuracy of our model on unseen data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4h7bXoW_2H3v",
        "outputId": "57ab293e-2dda-4ed2-f04f-a5894fbffa9e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.10852713178294 %\n",
            "Validation accuracy using random classifier: 16.884689922480618 %\n"
          ]
        }
      ],
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")\n",
        "\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier. Now let us try another random split and check the validation accuracy. We will see that the validation accuracy changes with the split. This is because the validation set is small and the accuracy is highly dependent on the samples in the validation set. We can get a better estimate of the accuracy by using cross-validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ujm3cyYzEntE",
        "outputId": "9e5727f3-06f1-4b33-faa9-b85db426271d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour algorithm: 34.048257372654156 %\n"
          ]
        }
      ],
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(\n",
        "    alltraindata, alltrainlabel, 75 / 100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour algorithm:\", valAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PNEZ5ToYBEDW",
        "outputId": "59cc7aca-0df7-4269-86f7-05ddd2270b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "\n",
        "print(\"Test accuracy:\", testAccuracy*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M9zvdYY6evgI"
      },
      "source": [
        "> Exercise: Try to implement a 3 nearest neighbour classifier and compare the accuracy of the 1 nearest neighbour classifier and the 3 nearest neighbour classifier on the test dataset. You can use the KNeighborsClassifier class from the scikit-learn library to implement the K-Nearest Neighbors model. You can set the number of neighbors using the n_neighbors parameter. You can also use the accuracy_score function from the scikit-learn library to calculate the accuracy of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>cross-validation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute. You can reduce the number of splits to make it faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "outputs": [],
      "source": [
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "    \"\"\"\n",
        "    This function takes in the data, labels, split percentage, number of iterations and classifier function\n",
        "    and returns the average accuracy of the classifier\n",
        "\n",
        "    alldata: numpy array of shape (n,d) where n is the number of samples and d is the number of features\n",
        "    alllabel: numpy array of shape (n,) where n is the number of samples\n",
        "    splitpercent: float which is the percentage of data to be used for training\n",
        "    iterations: int which is the number of iterations to run the classifier\n",
        "    classifier: function which is the classifier function to be used\n",
        "\n",
        "    returns: the average accuracy of the classifier\n",
        "    \"\"\"\n",
        "    accuracy = 0\n",
        "    for ii in range(iterations):\n",
        "        traindata, trainlabel, valdata, vallabel = split(\n",
        "            alldata, alllabel, splitpercent\n",
        "        )\n",
        "        valpred = classifier(traindata, trainlabel, valdata)\n",
        "        accuracy += Accuracy(vallabel, valpred)\n",
        "    return accuracy / iterations  # average of all accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3qtNar7Bbik",
        "outputId": "4d792fd3-7271-46fe-85a3-dc46d1bb4775"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy: 33.58463539517022 %\n",
            "Test accuracy: 34.91795366795367 %\n"
          ]
        }
      ],
      "source": [
        "avg_acc = AverageAccuracy(alltraindata, alltrainlabel, 75 / 100, 10, classifier=NN)\n",
        "print(\"Average validation accuracy:\", avg_acc*100, \"%\")\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "\n",
        "print(\"Test accuracy:\", Accuracy(testlabel, testpred)*100, \"%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1)Yes, averaging the validation accuracy across multiple splits (such as in **cross-validation**) generally provides more consistent and reliable results compared to using a single train-test split. Here’s why:\n",
        "\n",
        "### 1. **Reduces Variance**\n",
        "   - In a single train-test split, the performance of a model can be heavily influenced by how the data is split. The split might be random, leading to over- or underestimation of performance, especially if the dataset is small or imbalanced.\n",
        "   - By using multiple splits (e.g., in **k-fold cross-validation**), the model is trained and validated on different portions of the dataset, and the variance introduced by any specific data split is reduced.\n",
        "   - **Averaging the validation accuracy** across these multiple splits smooths out random variations that may come from particular splits, providing a more consistent estimate of the model's true performance.\n",
        "\n",
        "### 2. **Provides Better Generalization**\n",
        "   - When you average the validation accuracies across different folds, the resulting metric is a better approximation of how the model will perform on unseen data. This is because the model has been tested on several different validation sets, each time learning from a diverse subset of the data.\n",
        "   - This leads to a more robust estimate of generalization performance, which is crucial for understanding how well your model will perform on future data.\n",
        "\n",
        "### 3. **More Representative of the Entire Dataset**\n",
        "   - Cross-validation ensures that every data point is used for both training and validation, which provides a comprehensive view of how the model performs on the full dataset.\n",
        "   - For example, in **5-fold cross-validation**, each data point will appear in the validation set exactly once and in the training set four times. This comprehensive approach is more representative of the entire dataset compared to using just a single random split.\n",
        "\n",
        "### 4. **Helps Avoid Overfitting/Underfitting on Specific Splits**\n",
        "   - Sometimes, a model might perform unusually well on a specific train-test split due to overfitting, or poorly due to underfitting, because the training or validation data in that split may not be representative of the overall data distribution.\n",
        "   - Averaging over multiple splits reduces the chances of overfitting or underfitting to a particular subset of the data, leading to a more reliable estimate of the model's performance.\n",
        "\n",
        "### 5. **Stability in Results**\n",
        "   - With a single train-test split, slight changes in the random seed or split can result in noticeable variations in accuracy. This can make it difficult to determine if the model is genuinely performing well or if the results are due to randomness in the data split.\n",
        "   - By averaging validation accuracies across multiple splits, you reduce the likelihood of drawing conclusions based on a \"lucky\" or \"unlucky\" split, leading to more stable and reproducible results.\n",
        "\n",
        "### Example: **k-Fold Cross-Validation**\n",
        "   - In **k-fold cross-validation**, the dataset is split into \\(k\\) subsets (folds), and the model is trained \\(k\\) times, each time using a different fold as the validation set and the remaining \\(k-1\\) folds as the training set. The final accuracy is computed as the average of the \\(k\\) validation accuracies.\n",
        "\n",
        "   If you use **5-fold cross-validation**, the process works like this:\n",
        "   - Split the data into 5 folds.\n",
        "   - Train the model on 4 folds and validate on the remaining fold. Do this 5 times, each time rotating the validation fold.\n",
        "   - Calculate the validation accuracy for each fold.\n",
        "   - **Average the 5 validation accuracies** to get a final, more consistent accuracy estimate.\n",
        "\n",
        "### Mathematical View:\n",
        "The overall validation accuracy \\( A \\) for \\( k \\)-fold cross-validation is the average of the accuracies for each fold \\( A_i \\):\n",
        "\n",
        "\\[\n",
        "A = \\frac{1}{k} \\sum_{i=1}^{k} A_i\n",
        "\\]\n",
        "\n",
        "This process reduces the impact of any single fold where the validation accuracy might be unusually high or low due to random factors in the data split.\n",
        "\n",
        "### When to Use Averaging Across Splits:\n",
        "   - **Small Datasets**: When you have limited data, cross-validation is especially useful since it allows you to use all the data for training and testing.\n",
        "   - **Imbalanced Data**: Multiple splits help mitigate biases that may occur if an imbalanced class distribution ends up disproportionately in the training or test set in a single split.\n",
        "   - **Model Selection**: Averaging accuracies across multiple splits can provide a more stable metric to compare different models or hyperparameter configurations.\n",
        "\n",
        "### When to Be Cautious:\n",
        "   - If your dataset is extremely large, k-fold cross-validation may be computationally expensive, and in such cases, you might want to use a simpler technique like **train-test split** or **stratified sampling**.\n",
        "   - Ensure that data leakage is avoided by properly separating training and validation sets during each fold, especially in time series data or data with inherent relationships (e.g., hierarchical datasets).\n",
        "\n",
        "### Conclusion:\n",
        "Averaging the validation accuracy across multiple splits, such as in cross-validation, typically leads to more consistent and reliable estimates of model performance. It reduces variance, improves generalization, and provides a more accurate assessment of how the model will perform on unseen data.\n",
        "\n",
        "Would you like a demonstration of cross-validation with averaging, or do you want to explore a particular aspect further?"
      ],
      "metadata": {
        "id": "m6FsWcVexkdT"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-SBxy1qevgJ"
      },
      "source": [
        "> Exercise: How does the accuracy of the 3 nearest neighbour classifier change with the number of splits? How is it affected by the split size? Compare the results with the 1 nearest neighbour classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2)Yes, averaging the validation accuracy across multiple splits, such as in cross-validation, typically gives a **more accurate estimate of test accuracy** compared to a single train-test split. This is because cross-validation mitigates the variability that can arise from a single arbitrary split of the data. Here's why it works better:\n",
        "\n",
        "### 1. **Reduction of Bias and Variance**\n",
        "   - **Single Train-Test Split**: If you use a single train-test split, the model's performance can vary depending on how the data is divided. If the split is not representative of the overall dataset (e.g., if certain patterns or classes are overrepresented in the training or testing sets), it can lead to biased results.\n",
        "     - **High Variance**: The model's performance may fluctuate depending on the randomness of the split. One split may result in a high accuracy, while another might lead to a lower one.\n",
        "     - **Potential Bias**: If the validation set is not representative of the true data distribution, the model may seem to perform well (or poorly) on the validation set but not generalize well to unseen data.\n",
        "   \n",
        "   - **Cross-Validation**: In techniques like **k-fold cross-validation**, the data is split into \\(k\\) subsets, and the model is trained and validated \\(k\\) times, each time using a different subset for validation and the rest for training. This reduces both the bias and variance:\n",
        "     - **Low Bias**: Since every data point is used in both training and validation, the performance metric (e.g., accuracy) is more representative of the entire dataset.\n",
        "     - **Lower Variance**: By averaging the results across multiple folds, you smooth out the randomness and avoid over-reliance on a single arbitrary split.\n",
        "\n",
        "### 2. **Better Generalization to Test Data**\n",
        "   - In real-world scenarios, the ultimate goal is to understand how well the model will perform on **unseen (test) data**. Cross-validation provides a more accurate estimate of this by exposing the model to different subsets of the data and testing its ability to generalize across all parts of the dataset.\n",
        "   - Since cross-validation uses multiple validation sets from different parts of the data, it gives a more reliable estimate of test accuracy than relying on one test set in a single split.\n",
        "\n",
        "### 3. **More Robust Against Data Imbalances or Anomalies**\n",
        "   - In a single split, you might unintentionally have an imbalanced validation set, where certain classes or patterns are over- or under-represented. This can lead to an inaccurate estimate of test accuracy.\n",
        "   - In **stratified cross-validation**, each fold maintains the proportion of classes, ensuring a balanced validation set each time. This makes the accuracy estimates more reliable and representative of how the model will perform in the real world.\n",
        "   - Even in non-stratified k-fold cross-validation, any anomalies or imbalances in a particular split are averaged out across the multiple splits, leading to a more stable estimate.\n",
        "\n",
        "### 4. **More Representative Estimate of Model's Performance**\n",
        "   - By using all data points for both training and validation at different points in the cross-validation process, you are essentially testing how the model performs on **different subsets** of the data, which makes the accuracy estimate more **representative of the entire dataset**. This better simulates how the model will behave when exposed to new data.\n",
        "   - In contrast, with a single split, the test accuracy might depend too much on the specific characteristics of the data in that split, which may not generalize well to unseen data.\n",
        "\n",
        "### 5. **Handling Small Datasets**\n",
        "   - **Single Train-Test Split**: When the dataset is small, using a single train-test split can lead to a situation where the test set does not capture enough variation, leading to an unreliable estimate of test accuracy.\n",
        "   - **Cross-Validation**: With small datasets, cross-validation is particularly useful because it allows the model to be trained and validated on different subsets, making the most out of limited data. By averaging the accuracies across multiple splits, the estimate of test accuracy becomes more reliable, even with small datasets.\n",
        "\n",
        "### When Cross-Validation May Not Improve Test Accuracy Estimate:\n",
        "   While cross-validation generally gives a more accurate and consistent estimate of test accuracy, there are a few situations where it may not provide significant improvements:\n",
        "   \n",
        "   - **Large Datasets**: If your dataset is very large and representative, a single split may already give a good estimate of test accuracy, and cross-validation may not offer much additional benefit. However, even in this case, cross-validation could still improve the reliability of the accuracy estimate.\n",
        "   \n",
        "   - **Time Series or Sequential Data**: For time series or sequential data, regular k-fold cross-validation can introduce data leakage, where future information is used to predict the past. In such cases, techniques like **time series cross-validation** (where you maintain the temporal order in training and validation splits) are more appropriate.\n",
        "   \n",
        "   - **Computationally Expensive Models**: Cross-validation involves training the model multiple times, which can be computationally expensive. For complex models, this can be a limiting factor, and alternative approaches like a single train-test split may be chosen for practical reasons. However, the trade-off is that a single split gives a less reliable estimate of test accuracy.\n",
        "\n",
        "### Conclusion:\n",
        "Averaging validation accuracy across multiple splits (e.g., in cross-validation) usually gives a **more accurate and reliable estimate of test accuracy** than using a single train-test split. It reduces bias, variance, and the influence of any specific data split, leading to a more robust understanding of how well the model will generalize to unseen data.\n",
        "\n",
        "Would you like to see an implementation of cross-validation in practice or further details on a specific cross-validation method?"
      ],
      "metadata": {
        "id": "zEtjezxNyVGN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3)The number of iterations, particularly in the context of techniques like **k-fold cross-validation** or **bootstrapping**, can significantly impact the quality of the accuracy estimate. Here's how:\n",
        "\n",
        "### 1. **Higher Iterations in Cross-Validation (Increasing k)**\n",
        "   In **k-fold cross-validation**, the number of iterations is controlled by the number of folds \\( k \\). Increasing \\( k \\) (i.e., using more folds) typically gives a more accurate estimate of the model's performance, but there are trade-offs to consider.\n",
        "\n",
        "#### **Effects of Higher \\( k \\) on the Estimate:**\n",
        "   - **More Accurate and Stable Estimate**:\n",
        "     - As \\( k \\) increases, the model is trained and tested on larger and more varied subsets of the data. This leads to a more comprehensive understanding of how the model performs across different parts of the dataset, resulting in a more accurate and **stable** estimate of performance.\n",
        "     - In **Leave-One-Out Cross-Validation (LOO-CV)**, \\( k \\) is equal to the number of data points (i.e., you leave one data point out in each iteration). This method provides the least bias but can have **high variance**, especially with small datasets, since each fold is trained on almost the entire dataset and tested on just one data point.\n",
        "\n",
        "   - **Diminishing Returns with High \\( k \\)**:\n",
        "     - Although higher \\( k \\) values (like in 10-fold or 20-fold cross-validation) reduce variance and improve the estimate, beyond a certain point, the gains become minimal. Increasing \\( k \\) too much can lead to diminishing returns in accuracy improvement, while **significantly increasing computation time**.\n",
        "     - A common value is **5-fold** or **10-fold cross-validation**, which provides a good balance between estimate accuracy and computational cost.\n",
        "\n",
        "#### **Trade-Offs of Higher Iterations (Higher \\( k \\)):**\n",
        "   - **Computation Time**: More folds mean more training iterations. For instance, **10-fold cross-validation** requires training the model 10 times, while **5-fold cross-validation** requires only 5 iterations. This can become computationally expensive, especially for large datasets or complex models like deep learning networks.\n",
        "   - **Less Training Data Per Fold**: As \\( k \\) increases, the size of each training set becomes smaller (except for LOO-CV). This might make the model more sensitive to fluctuations in training data when working with small datasets, which could increase variance in some cases.\n",
        "\n",
        "### 2. **Iterations in Bootstrapping**\n",
        "   In **bootstrapping**, multiple samples are drawn with replacement from the dataset, and the model is trained and tested on these samples to estimate performance.\n",
        "\n",
        "#### **Effects of More Bootstrapping Iterations:**\n",
        "   - **More Stable Estimate**: Bootstrapping is inherently a stochastic process since samples are drawn randomly with replacement. As the number of iterations increases (i.e., more bootstrapped samples), the estimate of the performance metric (e.g., accuracy) becomes more stable and reliable because the variation across different samples averages out.\n",
        "   - **Diminishing Returns**: Similar to cross-validation, increasing the number of bootstrap iterations beyond a certain point results in diminishing returns. After a sufficient number of iterations (typically around 100 to 1,000, depending on the dataset size), the estimate becomes stable, and further iterations provide little improvement in accuracy but increase computation time.\n",
        "\n",
        "### 3. **General Impact of Higher Iterations on Accuracy Estimates**\n",
        "   - **Reduced Variance**: As the number of iterations (whether folds in cross-validation or samples in bootstrapping) increases, the variance in accuracy estimates decreases. This is because the performance estimate is averaged over multiple trials, reducing the impact of any single trial's randomness or peculiarities in the data.\n",
        "   - **Better Generalization Estimate**: A higher number of iterations often leads to a better approximation of the model's ability to generalize to unseen data. This is because more iterations provide a more thorough examination of how the model performs across different subsets of the data.\n",
        "\n",
        "### 4. **When Higher Iterations Might Not Help**\n",
        "   - **Data Size and Computational Cost**: If the dataset is very large and representative, a smaller number of iterations (like 5-fold cross-validation) might already provide a good estimate of test accuracy. In this case, increasing the number of iterations might only add computational cost without a significant gain in the estimate's accuracy.\n",
        "   - **Model Complexity**: For very complex models (e.g., deep neural networks), each iteration can be computationally expensive, and the benefit of more iterations may not justify the cost, especially if the estimate becomes stable early on.\n",
        "   - **Bias-Variance Trade-off**: In some cases, increasing iterations might reduce variance at the expense of introducing bias. For example, in Leave-One-Out Cross-Validation (where \\( k = N \\)), the model is trained on nearly all the data, which can reduce bias but increase variance, as the model may behave differently when tested on just one data point. This can lead to overfitting.\n",
        "\n",
        "### 5. **Optimal Number of Iterations**\n",
        "   - **Cross-Validation**: In practice, **5-fold** or **10-fold cross-validation** is often used because it strikes a balance between accuracy and computational efficiency. As \\( k \\) increases beyond 10, the benefits typically decrease while computational costs increase.\n",
        "   - **Bootstrapping**: The number of bootstrap iterations usually ranges from 100 to 1,000, depending on the dataset size and the desired level of stability in the estimate.\n",
        "\n",
        "### Conclusion:\n",
        "   - **Higher iterations generally lead to a better estimate** of accuracy by reducing the impact of random splits or samples, resulting in a more reliable and stable performance measure.\n",
        "   - However, increasing iterations beyond a certain point results in **diminishing returns**, where the improvement in accuracy estimate is minimal compared to the additional computational cost.\n",
        "   - A moderate number of iterations, such as **5- or 10-fold cross-validation**, or **100-1,000 bootstrap iterations**, is typically sufficient for most practical applications.\n",
        "\n",
        "Would you like to explore a specific number of iterations for a dataset or model? I can help demonstrate the effect or recommend an optimal setup based on your needs."
      ],
      "metadata": {
        "id": "J-FU5QM3zDEu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5)Increasing the number of iterations (whether in cross-validation or bootstrapping) **can help** mitigate the issue of having a very small training or validation dataset to some extent, but there are important limitations to consider. Here’s how it works and the potential trade-offs when dealing with small datasets:\n",
        "\n",
        "### 1. **Effect of Small Training Datasets**\n",
        "   - **Challenge**: A very small training dataset limits the model's ability to learn representative patterns from the data, which can lead to underfitting. The model may struggle to generalize because it hasn't seen enough data to form a robust understanding of the underlying patterns.\n",
        "   \n",
        "   - **Can Increasing Iterations Help?**:\n",
        "     - **Yes, but with limitations**: Increasing the number of iterations (e.g., using more folds in cross-validation or more bootstrap samples) will give the model more opportunities to see different parts of the dataset, which can reduce the variability in model performance. For example, in k-fold cross-validation, the model is trained multiple times on different splits, and each training set is different, so the model can potentially benefit from seeing different training data each time.\n",
        "     - However, if the training dataset is **very small**, no matter how many iterations you perform, the model may still struggle to capture complex patterns because the amount of data seen in each iteration will still be small.\n",
        "\n",
        "   - **Trade-Offs**:\n",
        "     - **Increased Variance**: When training data is very limited, increasing iterations can reduce bias but may also introduce high variance. This is because small changes in the data seen during each iteration can have a big impact on the model's performance. Models might become overly sensitive to the small training sets, leading to variability across iterations.\n",
        "     - **Diminishing Returns**: At a certain point, increasing iterations doesn't provide substantial new information. If the dataset is very small, seeing the same data over and over (just in slightly different configurations) won't dramatically improve the model’s learning.\n",
        "\n",
        "### 2. **Effect of Small Validation Datasets**\n",
        "   - **Challenge**: A small validation set leads to an unreliable estimate of performance because the validation set may not be representative of the overall data distribution. It can cause the model to appear to perform either better or worse than it would on a larger, more representative validation set.\n",
        "   \n",
        "   - **Can Increasing Iterations Help?**:\n",
        "     - **Yes**: Cross-validation or bootstrapping is especially useful when you have a small validation dataset. By rotating which subset of data is used for validation in each iteration, you give the model a more comprehensive evaluation across the entire dataset, leading to a more reliable performance estimate.\n",
        "     - For instance, in **k-fold cross-validation**, each data point will eventually be used as part of the validation set, so the final accuracy estimate is averaged over multiple validation sets, reducing the impact of a small validation set in any single iteration.\n",
        "\n",
        "   - **Trade-Offs**:\n",
        "     - **Computational Cost**: More iterations mean more computations, especially if each training run is expensive. For very small datasets, this might still be computationally feasible, but for larger models, it can become costly.\n",
        "     - **Risk of Overfitting**: With very small datasets, there’s a risk that increasing iterations may lead to overfitting, where the model starts to \"memorize\" small idiosyncrasies in the dataset rather than learning general patterns.\n",
        "\n",
        "### 3. **Leave-One-Out Cross-Validation (LOO-CV)**\n",
        "   - If the dataset is **extremely small**, you might consider **Leave-One-Out Cross-Validation (LOO-CV)**, where each data point is used as the validation set exactly once, and the remaining \\( N-1 \\) data points are used for training. This method provides the maximum possible number of iterations for a given dataset.\n",
        "   \n",
        "   - **Benefits**:\n",
        "     - LOO-CV maximizes the use of the data by training on nearly the entire dataset for each iteration and validating on a single point. It is especially useful when data is very limited, as it gives the model exposure to as much training data as possible in each iteration.\n",
        "   \n",
        "   - **Drawbacks**:\n",
        "     - **High Variance**: Since each iteration trains on almost the full dataset and tests on a single data point, the results can be highly variable, especially with small datasets.\n",
        "     - **Computational Cost**: LOO-CV can be computationally expensive because it requires training the model \\( N \\) times (where \\( N \\) is the number of data points).\n",
        "\n",
        "### 4. **Bootstrapping**\n",
        "   - Bootstrapping is another technique that can help with small datasets by creating multiple datasets through sampling with replacement. This way, the model can be trained on multiple variations of the data, providing a more robust performance estimate.\n",
        "   \n",
        "   - **Can Bootstrapping Help with Small Datasets?**\n",
        "     - **Yes**: Bootstrapping allows the model to learn from multiple \"resamples\" of the data, and since sampling is done with replacement, even small datasets can be used to generate many different training sets.\n",
        "     - **Limitations**: Like cross-validation, bootstrapping can give more reliable estimates by simulating multiple training-validation splits, but it can't overcome the fundamental limitation of insufficient training data. If the dataset is very small, each bootstrap sample will still contain much of the same data, just shuffled slightly.\n",
        "\n",
        "### 5. **Practical Approaches to Dealing with Small Datasets**\n",
        "   - **Data Augmentation**: In some cases, especially in fields like image processing or natural language processing, you can augment your small dataset by creating synthetic variations of your data (e.g., rotating images, adding noise, or paraphrasing text). This effectively increases the size of your dataset, improving both training and validation performance.\n",
        "   \n",
        "   - **Transfer Learning**: For very small datasets, consider using **transfer learning**, where you start with a model pre-trained on a large dataset and fine-tune it on your small dataset. This reduces the amount of data needed for training since the model has already learned useful features from a related task.\n",
        "   \n",
        "   - **Simpler Models**: With small datasets, simpler models (e.g., linear models or small decision trees) are often more effective than complex models like deep neural networks. Complex models tend to overfit on small datasets, while simpler models are more likely to generalize well with limited data.\n",
        "\n",
        "### Conclusion:\n",
        "Increasing the number of iterations (e.g., through cross-validation or bootstrapping) can help improve the estimate of performance when dealing with small train or validation datasets. However:\n",
        "   - **For small validation sets**: Increasing iterations helps significantly by giving a more reliable estimate of the model’s performance across different subsets of the data.\n",
        "   - **For small training sets**: Increasing iterations helps to some extent, but it **cannot fully compensate** for the lack of data. The model still needs enough training data to learn meaningful patterns, and additional iterations may only provide marginal gains.\n",
        "   \n",
        "Ultimately, while increasing iterations can help, it’s important to complement this with strategies like data augmentation, transfer learning, or simpler models to better handle small datasets.\n",
        "\n",
        "Would you like help with implementing one of these techniques, or would you like to explore another aspect of working with small datasets?"
      ],
      "metadata": {
        "id": "KBTjYDE9z6Zn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}